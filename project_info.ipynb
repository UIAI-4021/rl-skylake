{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "project phase 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Milad Tavakoli 4003623009\n",
    "Zahra Kazemi 4003653002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we defined a 2D table for quality of values and some variables that we calculate the value of some of them by trial and error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import gym_maze\n",
    "import numpy as np\n",
    "\n",
    "# Create an environment\n",
    "env = gym.make(\"maze-random-10x10-plus-v0\")\n",
    "observation = env.reset()\n",
    "\n",
    "qtable = np.zeros((100, 4))\n",
    "learning_rate = 0.8\n",
    "max_steps = 99\n",
    "gamma = 0.95\n",
    "epsilon = 1.0\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "decay_rate = 0.01\n",
    "NUM_EPISODES = 1000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this loop , we train the agent to access the final policy.at the first of each episod we reset the position of the agent then we convert the position into an integer using formula we had in previous phase of the project(observation=currentrowâˆ—ncols+currentcol)then agent will do the following step untill the game over or it reached the maximum possible steps:\n",
    "-by using (epsilon greedy algorithm)the action will be chosen\n",
    "-we get new state , reward and is_goal by taking the action\n",
    "-new value of Q will be calculate by Q_learning algorithm\n",
    "-new state will be replaced and all of these steps will repeat for new state too\n",
    "the epsilon value  updates for next episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(NUM_EPISODES):\n",
    "    env.render()\n",
    "    state = env.reset()\n",
    "    state = int(state[0] * 10 + state[1])\n",
    "    step = 0\n",
    "    done = False\n",
    "    for step in range(max_steps):\n",
    "        exp_exp_tradeoff = random.uniform(0, 1)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(qtable[state,:])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        new_state = int(new_state[0] * 10 + new_state[1])\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "        state = new_state\n",
    "        if done == True: \n",
    "            break\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
    "\n",
    "    next_state, reward, done, truncated = env.step(action)\n",
    "\n",
    "    if done or truncated:\n",
    "            observation = env.reset()\n",
    "\n",
    "    # Close the environment\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5742ae4c883d773d95e73662b3fb0f29de01ca7154801c8df9042e9dde4430c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
